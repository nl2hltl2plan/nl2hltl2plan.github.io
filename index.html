<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="NL2HLTL2PLAN: Scaling Up Natural Language Understanding for Multi-Robots Through Hierarchical Temporal Logic Task Representation">
    <meta name="keywords" content="Natural Language, Formal Method, Multi-Robots">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>NL2HLTL2PLAN: Scaling Up Natural Language Understanding for Multi-Robots Through Hierarchical Temporal Logic Task Representation</title>

    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>   -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-FV4ZJ9PVSV');
    </script>


    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-fullhd">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">NL2HLTL2PLAN: Scaling Up Natural Language Understanding for Multi-Robots Through Hierarchical Temporal Logic Task Representation</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a target="_blank" href="">Anonymous Authors</a>
                            </span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a target="_blank" href="xxx.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper (Coming soon)</span>
                                    </a>
                                </span>


                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a target="_blank" href="https://github.com/"
                                        class="external-link button is-normal is-rounded is-dark"
                                        rel="noopener noreferrer">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code (Coming soon)</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            <!-- To allow non-experts to easily specify long-horizon, multi-robot collaborative tasks, researchers are increasingly using language models to translate human natural language commands into formal specifications. However,because translation can occur in multiple ways, these specifications may not always be accurate nor result in efficient downstream multi-robot planning. Our core insight is that specifications should be concise representations, making it easier for downstream planners to find optimal solutions while remaining straightforward to derive from human instructions. Given the superior performance of multi-robot planners with hierarchical specifications, we represent tasks using hierarchical structures and introduce a full pipeline that translates natural language commands into hierarchical Linear Temporal Logic (LTL) and then solves the corresponding planning problem. The translation happens in two steps with the help of Large Language Models (LLMs).  
Initially, an LLM transforms the instructions into a hierarchical representation defined as Hierarchical Task Tree, capturing the logical and temporal relations among tasks. Following this, a fine-tuned LLM translates sub-tasks of each task into flat LTL formulas, aggregating them to form hierarchical LTL specifications. These specifications are then leveraged for planning using off-the-shelf planners. Our framework showcases the potential of the LLM in harnessing hierarchical reasoning to automate multi-robot task planning. Through evaluations in both simulation and real-world experiments involving human participants, we demonstrate that our method can handle more complex instructions compared to existing methods. Moreover, the results indicate that our approach achieves higher success rates and lower costs in multi-robot task allocation and plan generation. -->
    To enable non-experts to specify long-horizon, multi-robot collaborative tasks, language models are increasingly used to translate natural language commands into formal specifications. However, because translation can occur in multiple ways, such translations may lack accuracy or lead to inefficient multi-robot planning. Our key insight is that concise hierarchical specifications can simplify planning while remaining straightforward to derive from human instructions. We propose NL2HLTL2PLAN, a framework that translates natural language commands into hierarchical Linear Temporal Logic (LTL) and solves the corresponding planning problem. The translation involves two steps leveraging Large Language Models (LLMs). First, an LLM transforms instructions into a Hierarchical Task Tree, capturing logical and temporal relations. Next, a fine-tuned LLM converts sub-tasks into standard LTL formulas, which are aggregated into hierarchical specifications, with the lowest level corresponding to ordered robot actions. These specifications are then used with off-the-shelf planners. Our NL2HLTL2PLAN demonstrates the potential of LLMs in hierarchical reasoning for multi-robot task planning. Evaluations in simulation and real-world experiments with human participants show that NL2HLTL2PLAN outperforms existing methods, handling more complex instructions while achieving higher success rates and lower costs in task allocation and planning.
                            <!-- Long-horizon planning in robotics is often hindered by challenges such as uncertainty
                            accumulation, computational complexity, delayed rewards and incomplete information.
                            This work proposes an innovative approach to exploit the inherent task hierarchy from human
                            instructions to facilitate multi-robot planning.
                            Using Large Language Models (LLMs), we propose a two-step approach to translate
                            multi-sentence human instructions into a structured language,
                            Hierarchical Linear Temporal Logic (LTL), which serves as an intermediary formal
                            representation for planning.
                            Initially, LLMs transform the human instructions into a Hierarchical Task Network-like
                            representation, defined as Hierarchical Task Tree,
                            capturing the logical and temporal relations among tasks.
                            Following this, a domain-specific fine-tuning of LLM translates sub-tasks of each task into
                            flat LTL formulas,
                            aggregating them to form hierarchical LTL specifications.
                            These specifications are then leveraged for planning using off-the-shelf planners.
                            Our framework not only bridges the gap between human instructions and algorithmic planning
                            but also showcases the potential of LLMs in harnessing human-like hierarchical reasoning to
                            automate
                            complex task planning for multiple robots. Through evaluations in both simulation and
                            real-world
                            experiments involving human participants, we demonstrate that our method can handle more
                            complex human
                            instructions compared to existing methods. The results indicate that our approach achieves
                            higher success
                            rates and lower costs in multi-robot task allocation and plan generation. -->
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->

        </div>

        <!-- Paper video. -->
        <br>
        <br>

        <!-- <div class="container is-max-widescreen">

            <div class="rows">
                <h2 class="title is-3">Video</h2>
                <div class="publication-video">
                    <iframe src="media/videos/supp_material.mp4" frameborder="0" allow="autoplay; encrypted-media"
                        allowfullscreen></iframe>
                </div>
            </div>
        </div> -->
        <div class="container is-max-widescreen">
            <div class="rows">
                <!-- <h2 class="title is-3">Video</h2> -->
                <!-- <div class="publication-video">
                    <iframe src="https://youtu.be/o8CRrVK9g9" title="Scaling Up Natural Language Understanding for Multi-Robots Through the Lens of Hierarchy" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                </div> -->
                <div class="column has-text-centered">
                    <video id="dist1" controls muted autoplay loop width="99%">
                        <source src="media/videos/RAL-2-lowquality.mp4" type="video/mp4">
                    </video>
                    <p style="text-align:left">
                        demo video
                    </p>
                </div>
            </div>
        </div>

    </section>

    <section class="section">
        <div class="container is-max-widescreen">

            <div class="rows">


                <!-- Animation. -->
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dperact">Overview</span> </h2>

                        <!-- Interpolating. -->
                        <div class="content has-text-justified">
                            <!-- <br> -->
                        </div>
                        <img src="media/figures/overview.jpg" class="interpolation-image" alt="overview image"/>
                        </br>
                        </br>
                        <p class="content has-text-justified">
                            <em>
                                Overview of the framework, using the dishwasher loading problem as a case study. Note that the non-leaf nodes in the Hierarchical Task Tree (HTT), the language descriptions of sibling tasks, and the flat specifications are color-coded to indicate one-to-one correspondence.
                            </em>
                            
                        </p>
                        <p class="content has-text-justified">                        
                            The HTT tree is structured such that it unfolds level by level, where each child task is a decomposition of its parent task. Notably, the tasks at the bottom level are not necessarily indecomposable. This flexibility allows for varying numbers of levels and tasks per level, accommodating differences in task understanding and the range of primitive actions available to robot agents.
                        </p>
                        <p class="content has-text-justified">    
                            Additionally, it's important to highlight that the relation $R$ specifically captures the temporal relationships between sibling tasks that share the same parent. The temporal relationship between any two tasks in the tree can be inferred by tracing their lineage back to their common ancestor, thereby simplifying the overall complexity of the structure. 
                        </p>
                        <p class="content has-text-justified">    
                            When a task instruction is received, we use LLMs to construct the HTT through a two-step process, as outlined in <b>step 1</b>.
                            <ol>
                                <li><strong>Logical search:</strong> For every non-leaf node $v$, we gather its child tasks $V'$ and the temporal relations among them, defined by $R' \subseteq V' \times V'$. We then use LLMs to rephrase these child tasks and their temporal relations into syntactically correct sentences aligned with the semantics of LTL specifications (as illustrated in <b>step 2.1</b>). These reformulated sentences are input into a fine-tuned LLM that produces a single LTL formula (as depicted in <b>step 2.2</b>).
                                </li>
                                <li><strong>Action Completion:</strong> Given an HTT, each leaf node should represent a simple task on certain object, such as <em>"task 1.1.1 place plates into the lower rack"</em> in example. By viewing such simple task as a sequence of action steps, we prompt the LLM to generate a sequence of pre-defined API calls to expand the simple task. For instance, the symbol $\pi_{\text{plates}}^l$ that represents task 1.1.1 can be replaced with LTL specification composed of sequential APIs (<b>step 2.2</b>): $$\pi_{\text{plates}}^l = \Diamond ( \texttt{Pickup(plate)} \wedge \Diamond\, \texttt{Move(plate, lower_rack)})$$
                                After this step, a complete hierarchical LTL specifications is generated (example in <b>step 2.1</b>).
                                </li>
                            </ol>

                        </p>

                        </br>
                        <!--/ Re-rendering. -->
                    </div>
                </div>
    </section>


    <section class="section">
        <div class="container is-max-widescreen">

            <div class="rows">
                <!-- <h2 class="title is-3"></h2> -->
                <h2 class="title is-3">AI2-THOR experiments</h2>
                <p class="content has-text-justified">
                    <span>
                        To evaluate our method on tasks with more complex temporal requirements, we combine several base tasks in the The ALFRED dataset to generate <em>derivative</em> tasks (each derivative task can be composed with up to 4 base tasks).

                    </span>
                    </br>
                    </br>
                    <span>
                        We compare our method with <a href="https://arxiv.org/abs/2309.10062">SMART-LLM</a>. SMART-LLM uses LLMs to generate Python scripts that invoke predefined APIs for the purposes of task decomposition and task allocation. 
                    </span>    
                    </br>
                    </br> 
                    <span>
                        The metrics are as follows: 
                        <ol>
                            <li>
                                <strong>Success rate</strong>, which measures whether the target goal states of objects are achieved and if the order in which these states occur satisfies the specified temporal requirements.
                            </li>
                            <li>
                                <strong>Travel cost</strong>, measured in meters, is defined as the total distance traveled by all robots, excluding any costs related to manipulation. 
                            </li>
                            <li>
                                <strong>Completion time</strong>, quantified as the number of discrete time steps required to complete the tasks.
                            </li>
    
                        </ol>
                    </br>
                    </br>
                    </span>

                
                <div class="columns">
                    <div class="column has-text-centered">
                        <video id="dist1" controls muted autoplay loop width="99%">
                            <source src="media/videos/web-video-aithor-1task-1.mp4" type="video/mp4">
                        </video>
                        <p style="text-align:center">
                            <font color="green">1 base task with 4 robots</font>: Place a computer on the ottoman
                        </p>
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay loop width="99%">
                            <source src="media/videos/web-video-aithor-1task-2.mp4" type="video/mp4">
                        </video>
                        <p style="text-align:center">
                            <font color="green">1 base task with 4 robots</font>: Pick up a green candle and place it on the countertop
                        </p>
                    </div>

                </div>
                <!-- <h2 class="title is-5">Meta-Control finds the most approriate representations and control strategies for heterogeneous robot skills</h2> -->

                <span>
                    For derivatives tasks with 1 base task, our methods can effectively choose the shortest path while avoiding the repeating actions when executing sub-tasks, compared with the SMART-LLM which primarily seeks to identify a feasible solution without focusing on optimizing costs.
                </span>
                </br>
                </br>
                <div class="columns">
                    <div class="column has-text-centered">
                        <video id="dist1" controls muted autoplay loop width="99%">
                            <source src="media/videos/web-video-aithor-1task-3.mp4" type="video/mp4">
                        </video>
                        <p style="text-align:center">
                            <font color="green">4 base tasks with 1 robot in dinning room</font>
                        </p>
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay loop width="99%">
                            <source src="media/videos/web-video-aithor-4task-2.mp4" type="video/mp4">
                        </video>
                        <p style="text-align:center">
                            <font color="green">4 base tasks with 4 robots in dinning room</font>
                        </p>
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay width="99%">
                            <source src="media/videos/web-video-aithor-4task-3.mp4" type="video/mp4">
                        </video>
                        <p style="text-align:center">
                            <font color="green">4 base tasks with 4 robots in bedroom</font>
                        </p>
                    </div>
                </div>
                <span>
                    For derivative tasks with 4 base tasks, SMART-LLM failed to generate a feasible plan, as its prompt scripts always exceed the maximum content length the LLM accepts when the task content gradually becomes longer and more complex. However, as the HTT structure decomposes long tasks into tree-like dependencies, our method can effectively handle long tasks composed of multiple basic tasks, extract the temporal logic relationships, and provide planning. Here we demonstrated the planning of 4 robots under derivative tasks with 4 base tasks in two different environments.
                </span>
            </div>
        </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">

            <div class="rows">
                <!-- <h2 class="title is-3"></h2> -->
                <h2 class="title is-3">Real-world experiments</h2>
                <p class="content has-text-justified">
                    <span>
                        Our real-world experiments are conducted in a tabletop setting, where the task involves a robotic arm placing fruits and vegetables onto colored plates. 
                    </span>
                    </br>
                    </br>
                    <span>
                        Given the primarily 2D nature of the task, we convert the tabletop environment into a discrete grid world. The use of only one robotic arm simplifies the task compared to the multi-robot scenarios in the simulator, as it eliminates the need for task allocation. 

                    </span>
                    </br>
                    <span>
                        <ol>
                            Our evaluation focuses on two main aspects: 
                            <li>
                                the adaptability to different verbal tones and styles from various users;
                            </li>
                            <li>the comparative effectiveness of our plan solution against existing methods.

                            </li>
                        </ol>
                    </span>
                    
                    </br>
                    </br>
                <div class="columns">
                    <div class="column has-text-centered">
                        <video id="dist1" controls muted autoplay loop width="99%">
                            <source src="media/videos/5.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Wipe the whiteboard with a Cartesian trajectory planner.
                            Failed because force constraints can not be specified.
                        </p> -->
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay loop width="99%">
                            <source src="media/videos/8.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Open the cabinet with a joint space planner. Failed
                            because planned swing path is not accurate.
                        </p> -->
                    </div>
                </div>
                <span>
                    same task with different object arrangements
                </span>
                <div class="columns">
                    <div class="column has-text-centered">
                        <video id="dist1" controls muted autoplay loop width="99%">
                            <source src="media/videos/6_1.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Wipe the whiteboard with a Cartesian trajectory planner.
                            Failed because force constraints can not be specified.
                        </p> -->
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay loop width="99%">
                            <source src="media/videos/6_2.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Open the cabinet with a joint space planner. Failed
                            because planned swing path is not accurate.
                        </p> -->
                    </div>
                </div>
                <div class="columns">
                    <div class="column has-text-centered">
                        <video id="dist1" controls muted autoplay loop width="99%">
                            <source src="media/videos/7_1.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Wipe the whiteboard with a Cartesian trajectory planner.
                            Failed because force constraints can not be specified.
                        </p> -->
                    </div>

                    <div class="column has-text-centered">
                        <video id="dist2" controls muted autoplay loop width="99%">
                            <source src="media/videos/7_2.mp4.mp4" type="video/mp4">
                        </video>
                        <!-- <p style="text-align:center">
                            <font color="red">Failure: </font>Open the cabinet with a joint space planner. Failed
                            because planned swing path is not accurate.
                        </p> -->
                    </div>
                </div>
                <!-- <h2 class="title is-5">Meta-Control finds the most approriate representations and control strategies for heterogeneous robot skills</h2> -->
                <div class="columns">
                    <div class="column has-text-centered" >
                        <img src="media/figures/demo13.png" class="interpolation-image" style="width: 75%; height: auto;"/>
                    </div>
                </div>
            </br>
            <span>
                A sequence of images, arranged from left to right and top to bottom, depicts the task. "First, put a set of keychains on the armchair. Retrieve a pencil and put it on the side table. Move the phone and the bat to the bed in any order", objects and their trajectories are marked with different colors as follows, keychains (red), bat (blue), pencil (purple) and phone (green). $t$ represents the discrete time steps in simulation.              
            </span>
            </br>
                <div class="columns">
                    <div class="column has-text-centered" >
                        <img src="media/figures/four-arm-5.png" class="interpolation-image" style="width: 75%; height: auto;"/>
                    </div>
                </div>
            </br>
            <span>
                Snapshots depict four arms performing real-world tasks of picking and placing objects via handover. The instruction given is, "Please move the blue, green, and multi-colored blocks to the two opposite boxes, place the colored ones after the green ones."" Target areas are colored in magenta. The block being selected is emphasized with an ellipse, and the remaining blocks are contained within rectangles.           
            </span>
            </br>
                <div class="columns">
                    <div class="column has-text-centered" >
                        <img src="media/figures/our_real_world.png" class="interpolation-image" style="width: 75%; height: auto;"/>
                    </div>
                </div>
                <span>
                    The figure above is a comparative overview of snapshots between our approach and LLMs for task <em>"Place the green apple in the pink plate, the orange in the yellow plate and the red apple in the blue plate. The order of placement is not specified and can be chosen freely"</em>. 
                </span>
                </p>
                </br>
                <span>
                    Our method gives an optimal plan based on the positions of the fruits, whereas LLMs basically follow the sequence in which the fruits are mentioned in the instructions.                
                </span>
                </br>

                </br>
                <div class="columns">
                    <div class="column has-text-centered" >
                        <img src="media/figures/table.jpg" class="interpolation-image" style="width: 75%; height: auto;"/>
                    </div>
                </div>                  
                <span>
                    As observed, both our approach and the LLM achieve a high success rate, which aligns with the expectations given the task complexities. This confirms that our method is capable of adapting to various user instructions. Regarding cost, for the first four tasks that require sequential actions, the costs are identical. For the latter four tasks, which allow for multiple feasible solutions, our method consistently produces lower-cost paths, with the exception of task 5. In this simple task, the LLM also manages to create an optimal plan based on the placement of fruits.

                </span>
            </div>
        </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-widescreen">
            <div class="row">
                <h2 class="title is-3">
                    Prompts
                </h2>
                
                <p class="content has-text-justified">
                    Conversion from human instructions to Hierarchical Task Tree:
                    <br>
                    <a href="prompts/Prompt_for_generating_HTT_task_decomposition.html">Prompt for generating HTT task decomposition</a> |
                    <a href="prompts/Prompt_for_HTT_sibling_relationships.html">Prompt for extracting relationships between HTT sibling tasks</a> 

                </p>
                <p class="content has-text-justified">
                    Generation of task-wise flat LTL specifications:
                    <br>
                    <a href="prompts/Prompt_for_NL2TL.html">Nature language to LTL formula via a finetuned LLM</a> |
                    <a href="prompts/Prompt_for_sequence_AP.html">Prompt for action completion</a> |
                    <a href="prompts/complete_HLTL.html">An example of generated hierarchical LTL specifications</a> 
                </p>
    
                <p class="content has-text-justified">
                    Prompts for real-world experiments:
                    <br>
                    <a href="prompts/Prompt_for_LLMs.html">Prompt for LLMs to generate task plan</a> |
                    <a href="prompts/Prompt_for_multi-robot-LLMs.html">Prompt for LLMs to generate task plan for multi-robot handover task</a> 
                </p>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-widescreen">
            <div class="row">
                <h2 class="title is-3">
                    Instructions for Real-world Experiments
                </h2>

                <a href="prompts/real_world_task_1.html">A collection of representative tasks along with their rephrasings</a> 
                <br>
                <a href="prompts/real_world_multi_hand_over.html">Task description for multi-robot handover experiments</a> 
            </div>
        </div>
    </section>

    
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="row">
                <h2 class="title is-3">
                    Instructions for AI2-THOR Experiments
                </h2>
                <a href="prompts/aithor-4basetask.html">A list of these derivative tasks with 4 base tasks</a> 
                <br>
                <a href="https://github.com/darrrt/alfred_multiagent">A collection of derivative tasks composed of 1-4 base tasks</a> 

            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a
                                href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a
                                href="https://voxposer.github.io/">VoxPoser</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>
